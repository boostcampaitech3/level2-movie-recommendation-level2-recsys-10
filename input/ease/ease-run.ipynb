{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference code: https://github.com/franckjay/TorchEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random\n",
    "import logging\n",
    "import pandas as pd\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "from time import time\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # some cudnn methods can be random even after fixing the seed\n",
    "    # unless you tell it to be deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, path = '../data/', mode = 'train'):\n",
    "        self.path = path # default: '../data/'\n",
    "\n",
    "        #get number of users and items\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.neg_pools = {}\n",
    "        self.exist_users = []\n",
    "\n",
    "        # data_path는 사용자의 디렉토리에 맞게 설정해야 합니다.\n",
    "        data_path = os.path.join(self.path, 'train/train_ratings.csv')\n",
    "        genre_path = os.path.join(self.path, 'train/genres.tsv')\n",
    "        df = pd.read_csv(data_path)\n",
    "        genre_data = pd.read_csv(genre_path, sep='\\t')\n",
    "\n",
    "\n",
    "        ############### item based outlier ###############\n",
    "        # # 아이템 기준 outlier 제거 - 이용율 0.3% 미만인 아이템 제거 (영구히 제거)\n",
    "        # item_freq_df = (df.groupby('item')['user'].count()/df.user.nunique()).reset_index()\n",
    "        # item_freq_df.columns = ['item', 'item_freq']\n",
    "        # # df = df.merge(item_freq_df, on='item').query('item_freq > 0.003')\n",
    "        # df = df.merge(item_freq_df, on='item').query('item_freq > 0.005')\n",
    "        # # df = df.merge(item_freq_df, on='item').query('item_freq > 0.01')\n",
    "        # del df['item_freq'] # 소명을 다하고 삭제! \n",
    "\n",
    "        self.ratings_df = df.copy() # for submission\n",
    "        self.n_train = len(df)\n",
    "\n",
    "        item_ids = df['item'].unique() # 아이템 고유 번호 리스트\n",
    "        user_ids = df['user'].unique() # 유저 고유 번호 리스트\n",
    "        self.n_items, self.n_users = len(item_ids), len(user_ids)\n",
    "        \n",
    "        # user, item indexing\n",
    "        # item2idx = pd.Series(data=np.arange(len(item_ids))+1, index=item_ids) # item re-indexing (1~num_item) ; 아이템을 1부터 설정하는이유? 0을 아무것도 아닌 것으로 blank 하기 위해서\n",
    "        self.item2idx = pd.Series(data=np.arange(len(item_ids)), index=item_ids) # item re-indexing (0~num_item-1) ; 아이템을 1부터 설정하는이유? 0을 아무것도 아닌 것으로 blank 하기 위해서\n",
    "        self.user2idx = pd.Series(data=np.arange(len(user_ids)), index=user_ids) # user re-indexing (0~num_user-1)\n",
    "\n",
    "        # dataframe indexing\n",
    "        df = pd.merge(df, pd.DataFrame({'item': item_ids, 'item_idx': self.item2idx[item_ids].values}), on='item', how='inner')\n",
    "        df = pd.merge(df, pd.DataFrame({'user': user_ids, 'user_idx': self.user2idx[user_ids].values}), on='user', how='inner')\n",
    "        df.sort_values(['user_idx', 'time'], inplace=True)\n",
    "        genre_data = df.merge(genre_data, on = 'item').copy()\n",
    "        del df['item'], df['user'], genre_data['item'], genre_data['user']\n",
    "\n",
    "        self.exist_items = list(df['item_idx'].unique())\n",
    "        self.exist_users = list(df['user_idx'].unique())\n",
    "\n",
    "        ############### Used by Sampler ###############\n",
    "        # # 1. user-based outlier - 상위 20퍼센트 영화를 본 친구들 Weight=0 지정\n",
    "        # self.user_weights = np.ones_like(self.exist_users)\n",
    "        # outlier_users = df['user_idx'].unique()[df.groupby('user_idx').item_idx.count()/df['item_idx'].nunique() >= 0.4]\n",
    "        # self.user_weights[outlier_users] = 0\n",
    "\n",
    "        t1 = time()\n",
    "        self.train_items, self.valid_items = {}, {}\n",
    "        \n",
    "        items = df.groupby(\"user_idx\")[\"item_idx\"].apply(list) # 유저 아이디 상관 없이, 순서대로 \n",
    "        if mode == 'train':\n",
    "            print('Creating interaction Train/ Vaild Split...')\n",
    "            for uid, item in enumerate(items):            \n",
    "                num_u_valid_items = min(int(len(item)*0.125), 10) # 유저가 소비한 아이템의 12.5%, 그리고 최대 10개의 데이터셋을 무작위로 Validation Set으로 활용한다.\n",
    "                ####### Original method : RANDOM #######\n",
    "                # u_valid_items = np.random.choice(item, size=num_u_valid_items, replace=False)\n",
    "                # self.valid_items[uid] = u_valid_items\n",
    "                # self.train_items[uid] = list(set(item) - set(u_valid_items))\n",
    "\n",
    "                ####### method-1 : Last sequence ####### 마지막 sequence에 있는 정보를 제거\n",
    "                # u_valid_items = item[-num_u_valid_items:]\n",
    "                # self.valid_items[uid] = u_valid_items\n",
    "                # self.train_items[uid] = list(set(item) - set(u_valid_items))\n",
    "\n",
    "                ####### method-2 : hybrid ####### 마지막꺼:무작위= 1:1\n",
    "                # num_random = int(num_u_valid_items//2 + num_u_valid_items%2) # 홀수일때는, 무작위로 뽑는것이 1개 더 많게\n",
    "                # num_last = int(num_u_valid_items - num_random)\n",
    "                # last_items = item[-num_last:]\n",
    "                # random_items = np.random.choice(item[:-num_last], size=num_random, replace=False).tolist()\n",
    "                # u_valid_items = random_items + last_items\n",
    "                # self.valid_items[uid] = u_valid_items\n",
    "                # self.train_items[uid] = list(set(item) - set(u_valid_items))\n",
    "\n",
    "                ####### method-3 : hybrid ####### 마지막꺼:무작위= 6:4\n",
    "                num_random = np.floor(num_u_valid_items*0.6).astype(int) # 홀수일때는, 무작위로 뽑는것이 1개 더 많게\n",
    "                num_last = int(num_u_valid_items - num_random)\n",
    "                last_items = item[-num_last:]\n",
    "                random_items = np.random.choice(item[:-num_last], size=num_random, replace=False).tolist()\n",
    "                u_valid_items = random_items + last_items\n",
    "                self.valid_items[uid] = u_valid_items\n",
    "                self.train_items[uid] = list(set(item) - set(u_valid_items))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            self.train_data = pd.concat({k: pd.Series(v) for k, v in self.train_items.items()}).reset_index(0)\n",
    "            self.train_data.columns = ['user', 'item']\n",
    "\n",
    "            self.valid_data = pd.concat({k: pd.Series(v) for k, v in self.valid_items.items()}).reset_index(0)\n",
    "            self.valid_data.columns = ['user', 'item']\n",
    "        \n",
    "        if mode == 'train_all': #else\n",
    "            print('Preparing interaction all train set')\n",
    "            # for uid, item in enumerate(items):            \n",
    "            #     self.train_items[uid] = item\n",
    "\n",
    "            # self.train_data = pd.concat({k: pd.Series(v) for k, v in train_items.items()})\n",
    "            # self.train_data.reset_index(0, inplace=True)\n",
    "            # self.train_data.columns = ['user', 'item']\n",
    "            self.train_data = pd.DataFrame()\n",
    "            self.train_data['user'] = df['user_idx']\n",
    "            self.train_data['item'] = df['item_idx']\n",
    "\n",
    "        print('Train/Vaild Split Complete. Takes in', time() - t1, 'sec')\n",
    "        \n",
    "        rows, cols = self.train_data['user'], self.train_data['item']\n",
    "        self.train_input_data = sp.csr_matrix(\n",
    "            (np.ones_like(rows), (rows, cols)), \n",
    "            dtype='float32',\n",
    "            shape=(self.n_users, self.n_items))\n",
    "        self.train_input_data = self.train_input_data.toarray()\n",
    "\n",
    "\n",
    "        print('Making Genre filter ... ')\n",
    "        genre2item = genre_data.groupby('genre')['item_idx'].apply(set).apply(list)\n",
    "        # user2genre = genre_data.groupby('user_idx')['genre'].apply(set).apply(list)\n",
    "\n",
    "        genre_data_freq = genre_data.groupby('user_idx')['genre'].value_counts(normalize=True)\n",
    "        genre_data_freq_over_5p = genre_data_freq[genre_data_freq > 0.003].reset_index('user_idx')\n",
    "        genre_data_freq_over_5p.columns = ['user_idx', 'tobedroped']\n",
    "        genre_data_freq_over_5p = genre_data_freq_over_5p.drop('tobedroped', axis = 1).reset_index()\n",
    "        user2genre = genre_data_freq_over_5p.groupby('user_idx')['genre'].apply(set).apply(list)\n",
    "\n",
    "        genre2item_dict = genre2item.to_dict()\n",
    "        all_set_genre = set(genre_data['genre'].unique())\n",
    "        user_genre_filter_dict = {}\n",
    "        for user, genres in tqdm(enumerate(user2genre)):\n",
    "            unseen_genres = all_set_genre - set(genres) # set\n",
    "            unseen_genres_item = set(sum([genre2item_dict[genre] for genre in unseen_genres], []))\n",
    "            user_genre_filter_dict[user] = pd.Series(list(unseen_genres_item), dtype=np.int32)\n",
    "\n",
    "        user_genre_filter_df = pd.concat(user_genre_filter_dict).reset_index(0)\n",
    "        user_genre_filter_df.columns = ['user', 'item']\n",
    "        user_genre_filter_df.index = range(len(user_genre_filter_df))\n",
    "\n",
    "        rows, cols = user_genre_filter_df['user'], user_genre_filter_df['item']\n",
    "        self.user_genre_filter = sp.csr_matrix(\n",
    "            (np.ones_like(rows), (rows, cols)), \n",
    "            dtype='float32',\n",
    "            shape=(self.n_users, self.n_items))\n",
    "        self.user_genre_filter = self.user_genre_filter.toarray()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_users\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.train_input_data[idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchEASE:\n",
    "    def __init__(\n",
    "        self, train, user_col=\"user_id\", item_col=\"item_id\", score_col=None, reg=250.0, dataset = None):\n",
    "        \"\"\"\n",
    "        :param train: Training DataFrame of user, item, score(optional) values\n",
    "        :param user_col: Column name for users\n",
    "        :param item_col: Column name for items\n",
    "        :param score_col: Column name for scores. Implicit feedback otherwise\n",
    "        :param reg: Regularization parameter.\n",
    "                    Change by orders of magnitude to tune (2e1, 2e2, ...,2e4)\n",
    "        \"\"\"\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s [%(levelname)s] %(name)s - %(message)s\",\n",
    "            level=logging.INFO,\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "            stream=sys.stdout,\n",
    "        )\n",
    "        if dataset:\n",
    "            self.dataset = dataset\n",
    "\n",
    "        self.logger = logging.getLogger(\"notebook\")\n",
    "        self.logger.info(\"Building user + item lookup\")\n",
    "        # How much regularization do you need?\n",
    "        self.reg = reg\n",
    "\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "\n",
    "        self.user_id_col = user_col + \"_id\"\n",
    "        self.item_id_col = item_col + \"_id\"\n",
    "\n",
    "        self.user_lookup = self.generate_labels(train, self.user_col)\n",
    "        self.item_lookup = self.generate_labels(train, self.item_col)\n",
    "\n",
    "        self.item_map = {}\n",
    "        self.logger.info(\"Building item hashmap\")\n",
    "        for _item, _item_id in self.item_lookup.values:\n",
    "            self.item_map[_item_id] = _item\n",
    "\n",
    "        train = pd.merge(train, self.user_lookup, on=[self.user_col])\n",
    "        train = pd.merge(train, self.item_lookup, on=[self.item_col])\n",
    "        self.logger.info(\"User + item lookup complete\")\n",
    "        self.indices = torch.LongTensor(\n",
    "            train[[self.user_id_col, self.item_id_col]].values\n",
    "        )\n",
    "\n",
    "        if not score_col:\n",
    "            # Implicit values only\n",
    "            self.values = torch.ones(self.indices.shape[0])\n",
    "        else:\n",
    "            # TODO: Test if score_col works correctly\n",
    "            self.values = torch.FloatTensor(train[score_col])\n",
    "\n",
    "        # TODO: Is Sparse the best implementation?\n",
    "        self.sparse = torch.sparse.FloatTensor(self.indices.t(), self.values)\n",
    "\n",
    "        self.logger.info(\"Sparse data built\")\n",
    "\n",
    "    def generate_labels(self, df, col):\n",
    "        dist_labels = df[[col]].drop_duplicates()\n",
    "        dist_labels[col + \"_id\"] = dist_labels[col].astype(\"category\").cat.codes\n",
    "\n",
    "        return dist_labels\n",
    "\n",
    "    def fit(self):\n",
    "        self.logger.info(\"Building G Matrix\")\n",
    "        G = self.sparse.to_dense().t() @ self.sparse.to_dense()\n",
    "        G += torch.eye(G.shape[0]) * self.reg\n",
    "\n",
    "        P = G.inverse()\n",
    "\n",
    "        self.logger.info(\"Building B matrix\")\n",
    "        B = P / (-1 * P.diag())\n",
    "        # Set diagonals to 0. TODO: Use .fill_diag_\n",
    "        B = B + torch.eye(B.shape[0])\n",
    "\n",
    "        # Predictions for user `_u` will be self.sparse.to_dense()[_u]@self.B\n",
    "        self.B = B\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict_all(self, pred_df, k=5, remove_owned=True, genre_filter = False):\n",
    "        \"\"\"\n",
    "        :param pred_df: DataFrame of users that need predictions\n",
    "        :param k: Number of items to recommend to each user\n",
    "        :param remove_owned: Do you want previously interacted items included?\n",
    "        :return: DataFrame of users + their predictions in sorted order\n",
    "        \"\"\"\n",
    "        pred_df = pred_df[[self.user_col]].drop_duplicates()\n",
    "        n_orig = pred_df.shape[0]\n",
    "\n",
    "        # Alert to number of dropped users in prediction set\n",
    "        pred_df = pd.merge(pred_df, self.user_lookup, on=[self.user_col])\n",
    "        n_curr = pred_df.shape[0]\n",
    "        if n_orig - n_curr:\n",
    "            self.logger.info(\n",
    "                \"Number of unknown users from prediction data = %i\" % (n_orig - n_curr)\n",
    "            )\n",
    "\n",
    "        _output_preds = []\n",
    "        # Select only user_ids in our user data\n",
    "        _user_tensor = self.sparse.to_dense().index_select(\n",
    "            dim=0, index=torch.LongTensor(pred_df[self.user_id_col])\n",
    "        )\n",
    "\n",
    "        # Make our (raw) predictions\n",
    "        _preds_tensor = _user_tensor @ self.B\n",
    "        self.logger.info(\"Predictions are made\")\n",
    "        if remove_owned:\n",
    "            # Discount these items by a large factor (much faster than list comp.)\n",
    "            self.logger.info(\"Removing owned items\")\n",
    "            _preds_tensor += -1.0 * _user_tensor\n",
    "\n",
    "        if genre_filter:\n",
    "            self.logger.info(\"Removing never seen genre movies\")\n",
    "            _preds_tensor += -1.0 * self.dataset.user_genre_filter\n",
    "\n",
    "\n",
    "        self.logger.info(\"TopK selected per user\")\n",
    "        for _preds in _preds_tensor:\n",
    "            # Very quick to use .topk() vs. argmax()\n",
    "            _output_preds.append(\n",
    "                [self.item_map[_id] for _id in _preds.topk(k).indices.tolist()]\n",
    "            )\n",
    "\n",
    "        pred_df[\"predicted_items\"] = _output_preds\n",
    "        self.logger.info(\"Predictions are returned to user\")\n",
    "        return pred_df\n",
    "\n",
    "    def score_predictions(self):\n",
    "        # TODO: Implement this with some common metrics\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interaction Train/ Vaild Split...\n",
      "Train/Vaild Split Complete. Takes in 20.065412759780884 sec\n",
      "Making Genre filter ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31360it [00:06, 4728.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 02:59:31 [INFO] notebook - Building user + item lookup\n",
      "2022-04-14 02:59:31 [INFO] notebook - Building item hashmap\n",
      "2022-04-14 02:59:32 [INFO] notebook - User + item lookup complete\n",
      "2022-04-14 02:59:32 [INFO] notebook - Sparse data built\n",
      "2022-04-14 02:59:32 [INFO] notebook - Building G Matrix\n",
      "2022-04-14 02:59:42 [INFO] notebook - Building B matrix\n"
     ]
    }
   ],
   "source": [
    "# parameter setting\n",
    "reg = 600\n",
    "is_genre_filter = False\n",
    "score_col=None\n",
    "k = 10\n",
    "\n",
    "# dataset setting\n",
    "dataset = BaseDataset(path = '../data/', mode = 'train')\n",
    "\n",
    "# model setting\n",
    "model = TorchEASE(dataset.train_data, user_col='user', item_col='item', score_col=score_col, reg=reg, dataset = dataset)\n",
    "\n",
    "# fit\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 02:59:49 [INFO] notebook - Predictions are made\n",
      "2022-04-14 02:59:49 [INFO] notebook - Removing owned items\n",
      "2022-04-14 02:59:49 [INFO] notebook - TopK selected per user\n",
      "2022-04-14 02:59:50 [INFO] notebook - Predictions are returned to user\n",
      "0.1588710089690593\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "output = model.predict_all(dataset.train_data, k=k, genre_filter = is_genre_filter)\n",
    "solution = output.drop('user_id', axis=1).set_index('user')['predicted_items']\n",
    "answer = dataset.valid_data.groupby('user')['item'].apply(list)\n",
    "\n",
    "# Get Recall@10\n",
    "recall = 0.0\n",
    "for ans, sol in zip(answer, solution):\n",
    "    # print(ans, sol)    \n",
    "    ans_set = set(ans)\n",
    "    sol_set = set(sol)\n",
    "    denominator = min(10, len(ans))\n",
    "    numerator = len(ans_set.intersection(sol_set))\n",
    "    recall += numerator/denominator\n",
    "    \n",
    "recall = recall/len(answer)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing interaction all train set\n",
      "Train/Vaild Split Complete. Takes in 2.2489964962005615 sec\n",
      "Making Genre filter ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31360it [00:06, 4637.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-14 03:00:25 [INFO] notebook - Building user + item lookup\n",
      "2022-04-14 03:00:25 [INFO] notebook - Building item hashmap\n",
      "2022-04-14 03:00:26 [INFO] notebook - User + item lookup complete\n",
      "2022-04-14 03:00:26 [INFO] notebook - Sparse data built\n",
      "2022-04-14 03:00:26 [INFO] notebook - Building G Matrix\n",
      "2022-04-14 03:00:36 [INFO] notebook - Building B matrix\n",
      "2022-04-14 03:00:43 [INFO] notebook - Predictions are made\n",
      "2022-04-14 03:00:43 [INFO] notebook - Removing owned items\n",
      "2022-04-14 03:00:43 [INFO] notebook - TopK selected per user\n",
      "2022-04-14 03:00:45 [INFO] notebook - Predictions are returned to user\n"
     ]
    }
   ],
   "source": [
    "###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "###### ###### ###### ###### ###### ######    submission area   ###### ###### ###### ###### ###### ###### ######\n",
    "###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "\n",
    "submission_dataset = BaseDataset(path = '../data/', mode = 'train_all')\n",
    "submission_model = TorchEASE(submission_dataset.train_data, user_col='user', item_col='item', score_col=None, reg=600)\n",
    "submission_model.fit()\n",
    "submission_output = submission_model.predict_all(submission_dataset.train_data, k=10)\n",
    "\n",
    "submission_output = pd.concat({k: pd.Series(v) for k, v in submission_output.drop('user_id', axis = 1).set_index('user')['predicted_items'].items()}).reset_index(0)\n",
    "submission_output.columns = ['user', 'item']\n",
    "\n",
    "idx2item = submission_dataset.item2idx.reset_index()\n",
    "idx2item.columns = ['item', 'item_idx']\n",
    "idx2item = idx2item.set_index('item_idx')\n",
    "\n",
    "idx2user = submission_dataset.user2idx.reset_index()\n",
    "idx2user.columns = ['user', 'user_idx']\n",
    "idx2user = idx2user.set_index('user_idx')\n",
    "\n",
    "submission_output['item'] = submission_output['item'].replace(idx2item.to_dict()['item'])\n",
    "submission_output['user'] = submission_output['user'].replace(idx2user.to_dict()['user'])\n",
    "submission_output = submission_output.sort_values('user')\n",
    "submission_output.to_csv(\"submission_ease.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8892ab59d46dba3f4efad217c937d392d78b127da621344609ec3a012e116b8b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import easydict\n",
    "import tqdm\n",
    "import scipy.sparse as sp\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for i in set([10, 20,30]) - set([10]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4., 5., 6.],\n",
      "        [1., 2., 3., 4., 5., 7.]])\n",
      "tensor([[1., 2., 3., 4., 5., 6.],\n",
      "        [1., 2., 3., 4., 5., 7.]])\n",
      "tensor([[1., 2., 3., 4., 5., 6.],\n",
      "        [1., 2., 3., 4., 5., 6.],\n",
      "        [1., 2., 3., 4., 5., 6.],\n",
      "        [1., 2., 3., 4., 5., 6.],\n",
      "        [1., 2., 3., 4., 5., 6.]], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1,2,3,4,5,6], [1,2,3,4,5,7]])\n",
    "print(x)\n",
    "b = nn.Parameter(x)\n",
    "print(x)\n",
    "print(b[np.array([0,0,0,0,0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to utils\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # some cudnn methods can be random even after fixing the seed\n",
    "    # unless you tell it to be deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "        {\"data_dir\": './data/', \n",
    "         \"dataset\": \"ml-100k_ngcf\", \n",
    "         \"results_dir\": '', \n",
    "         \"n_epochs\" : 40,# 400,\n",
    "         \"reg\" : 1e-5,\n",
    "         \"lr\" : 0.0001,\n",
    "         \"emb_dim\" : 64,\n",
    "         \"layers\" : '[64, 64]',\n",
    "         \"batch_size\" : 1024,\n",
    "         \"node_dropout\" : 0.1,\n",
    "         \"mess_dropout\" : 0.1,\n",
    "         \"k\" : 20, # 'k order of metric evaluation (e.g. NDCG@k)\n",
    "         \"eval_N\" : 1,\n",
    "         \"save_results\" : 1,\n",
    "         \"gpu_id\" : 0 ,\n",
    "         })\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(args.gpu_id) if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read parsed arguments\n",
    "data_dir = args.data_dir\n",
    "dataset = args.dataset\n",
    "batch_size = args.batch_size\n",
    "layers = eval(args.layers)\n",
    "emb_dim = args.emb_dim\n",
    "lr = args.lr\n",
    "reg = args.reg\n",
    "mess_dropout = args.mess_dropout\n",
    "node_dropout = args.node_dropout\n",
    "k = args.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path는 사용자의 디렉토리에 맞게 설정해야 합니다.\n",
    "data_path = '../data/train/train_ratings.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "item_ids = df['item'].unique() # 아이템 고유 번호 리스트\n",
    "user_ids = df['user'].unique() # 유저 고유 번호 리스트\n",
    "num_item, num_user = len(item_ids), len(user_ids)\n",
    "num_batch = num_user // batch_size\n",
    "\n",
    "# user, item indexing\n",
    "# item2idx = pd.Series(data=np.arange(len(item_ids))+1, index=item_ids) # item re-indexing (1~num_item) ; 아이템을 1부터 설정하는이유? 0을 아무것도 아닌 것으로 blank 하기 위해서\n",
    "item2idx = pd.Series(data=np.arange(len(item_ids)), index=item_ids) # item re-indexing (0~num_item-1) ; 아이템을 1부터 설정하는이유? 0을 아무것도 아닌 것으로 blank 하기 위해서\n",
    "user2idx = pd.Series(data=np.arange(len(user_ids)), index=user_ids) # user re-indexing (0~num_user-1)\n",
    "\n",
    "# dataframe indexing\n",
    "df = pd.merge(df, pd.DataFrame({'item': item_ids, 'item_idx': item2idx[item_ids].values}), on='item', how='inner')\n",
    "df = pd.merge(df, pd.DataFrame({'user': user_ids, 'user_idx': user2idx[user_ids].values}), on='user', how='inner')\n",
    "df.sort_values(['user_idx', 'time'], inplace=True)\n",
    "del df['item'], df['user'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_item:  6807\n",
      "num_user:  31360\n"
     ]
    }
   ],
   "source": [
    "print('num_item: ', num_item)\n",
    "print('num_user: ', num_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31360it [00:55, 560.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Interaction matrices R_train and R_test created in 57.48108887672424 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create interactions/ratings matrix 'R' # dok = dictionary of keys\n",
    "R_train = sp.dok_matrix((num_user, num_item), dtype=np.float32) \n",
    "train_items, test_set = {}, {}\n",
    "\n",
    "t1 = time() # timer\n",
    "items = df.groupby(\"user_idx\")[\"item_idx\"].apply(list) # 유저 아이디 상관 없이, 순서대로 \n",
    "for uid, item in tqdm.tqdm(enumerate(items)):\n",
    "    for i in item:\n",
    "        R_train[uid, i] = 1.\n",
    "    train_items[uid] = item\n",
    "\n",
    "print('Complete. Interaction matrices R_train and R_test created in', time() - t1, 'sec')\n",
    "\n",
    "\n",
    "# if exist, get adjacency matrix\n",
    "def get_adj_mat(self):\n",
    "    try:\n",
    "        t1 = time()\n",
    "        adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "        print('Loaded adjacency-matrix (shape:', adj_mat.shape,') in', time() - t1, 'sec.')\n",
    "\n",
    "    except Exception:\n",
    "        print('Creating adjacency-matrix...')\n",
    "        adj_mat = self.create_adj_mat()\n",
    "        sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "    return adj_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0,1,2,3,4][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating adjacency-matrix...\n",
      "(31360, 6807)\n",
      "Complete. Adjacency-matrix created in (38167, 38167) 32.98339104652405 sec.\n",
      "Transforming adjacency-matrix to Normalized-adjacency matrix...\n",
      "Complete. Transformed adjacency-matrix to Normalized-adjacency matrix in 8.019442319869995 sec.\n"
     ]
    }
   ],
   "source": [
    "# # if exist, get adjacency matrix\n",
    "# def get_adj_mat(self):\n",
    "#     try:\n",
    "#         t1 = time()\n",
    "#         adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "#         print('Loaded adjacency-matrix (shape:', adj_mat.shape,') in', time() - t1, 'sec.')\n",
    "\n",
    "#     except Exception:\n",
    "#         print('Creating adjacency-matrix...')\n",
    "#         adj_mat = self.create_adj_mat()\n",
    "#         sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "#     return adj_mat\n",
    "\n",
    "# # create adjancency matrix\n",
    "# def create_adj_mat(self):\n",
    "#     t1 = time()\n",
    "    \n",
    "#     adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "#     adj_mat = adj_mat.tolil()\n",
    "#     R = self.R_train.tolil() # to list of lists\n",
    "\n",
    "#     adj_mat[:self.n_users, self.n_users:] = R\n",
    "#     adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "#     adj_mat = adj_mat.todok()\n",
    "#     print('Complete. Adjacency-matrix created in', adj_mat.shape, time() - t1, 'sec.')\n",
    "\n",
    "#     t2 = time()\n",
    "\n",
    "#     # normalized adjacency matrix\n",
    "#     def normalized_adj_single(adj):\n",
    "        \n",
    "#         ### 논문 수식 (8) ###\n",
    "#         rowsum = np.array(adj.sum(1))                \n",
    "#         d_inv = np.power(rowsum, -.5).flatten()     \n",
    "#         d_inv[np.isinf(d_inv)] = 0.\n",
    "#         d_mat_inv = sp.diags(d_inv)                 \n",
    "#         norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv) \n",
    "        \n",
    "#         return norm_adj.tocoo()\n",
    "\n",
    "#     print('Transforming adjacency-matrix to Normalized-adjacency matrix...')\n",
    "#     ngcf_adj_mat = normalized_adj_single(adj_mat)\n",
    "\n",
    "#     print('Complete. Transformed adjacency-matrix to Normalized-adjacency matrix in', time() - t2, 'sec.')\n",
    "#     return ngcf_adj_mat.tocsr()\n",
    "\n",
    "def create_adj_mat():\n",
    "    t1 = time()\n",
    "    \n",
    "    adj_mat = sp.dok_matrix((num_user + num_item, num_user + num_item), dtype=np.float32)\n",
    "    adj_mat = adj_mat.tolil()\n",
    "    R = R_train.tolil() # to list of lists\n",
    "\n",
    "    adj_mat[:num_user, num_user:] = R\n",
    "    adj_mat[num_user:, :num_user] = R.T\n",
    "    adj_mat = adj_mat.todok()\n",
    "    print('Complete. Adjacency-matrix created in', adj_mat.shape, time() - t1, 'sec.')\n",
    "\n",
    "    t2 = time()\n",
    "\n",
    "    # normalized adjacency matrix\n",
    "    def normalized_adj_single(adj):\n",
    "        \n",
    "        ### 논문 수식 (8) ###\n",
    "        rowsum = np.array(adj.sum(1))                \n",
    "        d_inv = np.power(rowsum, -.5).flatten()     \n",
    "        d_inv[np.isinf(d_inv)] = 0.\n",
    "        d_mat_inv = sp.diags(d_inv)                 \n",
    "        norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv) \n",
    "        \n",
    "        return norm_adj.tocoo()\n",
    "\n",
    "    print('Transforming adjacency-matrix to Normalized-adjacency matrix...')\n",
    "    ngcf_adj_mat = normalized_adj_single(adj_mat)\n",
    "\n",
    "    print('Complete. Transformed adjacency-matrix to Normalized-adjacency matrix in', time() - t2, 'sec.')\n",
    "    return ngcf_adj_mat.tocsr()\n",
    "\n",
    "\n",
    "print('Creating adjacency-matrix...')\n",
    "adj_mat = create_adj_mat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refresh negative pools 16.68658709526062\n"
     ]
    }
   ],
   "source": [
    "# create collections of N items that users never interacted with\n",
    "neg_pools = {} # self.neg_pools = {}\n",
    "def negative_pool():\n",
    "    t1 = time()\n",
    "    for u in train_items.keys():\n",
    "        neg_items = list(set(range(num_item)) - set(train_items[u]))\n",
    "        pools = [random.choice(neg_items) for _ in range(100)]\n",
    "        neg_pools[u] = pools\n",
    "    print('refresh negative pools', time() - t1)\n",
    "negative_pool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data for mini-batches\n",
    "def sample(self):\n",
    "    # 배치 사이즈 = 유저의 수\n",
    "    if self.batch_size <= self.n_users:\n",
    "        users = random.sample(self.exist_users, self.batch_size)\n",
    "    else:\n",
    "        users = [random.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "\n",
    "    # 유저 u에 대해서 positive item 실제로 본 영화도 다 넣는게 아니다. 매번 샘플링으로 결정한다. \n",
    "    def sample_pos_items_for_u(u, num):\n",
    "        pos_items = self.train_items[u]\n",
    "        n_pos_items = len(pos_items)\n",
    "        pos_batch = []\n",
    "        while True:\n",
    "            if len(pos_batch) == num: break\n",
    "            pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "            pos_i_id = pos_items[pos_id]\n",
    "\n",
    "            if pos_i_id not in pos_batch:\n",
    "                pos_batch.append(pos_i_id)\n",
    "        return pos_batch\n",
    "\n",
    "    # 유저 u가 보지 않았던 영화들을 num개 만큼 뽑는다.\n",
    "    def sample_neg_items_for_u(u, num):\n",
    "        neg_items = []\n",
    "        while True:\n",
    "            if len(neg_items) == num: break\n",
    "            neg_id = np.random.randint(low=0, high=self.n_items -1 ,size=1)[0]\n",
    "            if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                neg_items.append()\n",
    "        return neg_items\n",
    "\n",
    "    # \n",
    "    def sample_neg_items_for_u_from_pools(u, num):\n",
    "        neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "        return random.sample(neg_items, num)\n",
    "\n",
    "    # (batch size 만큼의) 유저에 대해서, 그 유저들의 pos item과 neg item 리스트를 만든다.    \n",
    "    pos_items, neg_items = [], []\n",
    "    for u in users:\n",
    "        pos_items += sample_pos_items_for_u(u, 1)\n",
    "        neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "    return users, pos_items, neg_items\n",
    "\n",
    "def get_num_users_items(self):\n",
    "    return self.n_users, self.n_items\n",
    "\n",
    "def print_statistics(self):\n",
    "    print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
    "    print('n_interactions=%d' % (self.n_train + self.n_test))\n",
    "    print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 주어진 결과와 정확히 비교하기 위한 random seed 고정\n",
    "###############################################################################\n",
    "\n",
    "seed = 0  # 바꾸지 마시오!\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda'), True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device, use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "        {\"data_dir\": './data/', \n",
    "         \"dataset\": \"ml-100k_ngcf\", \n",
    "         \"results_dir\": '', \n",
    "         \"n_epochs\" : 40,# 400,\n",
    "         \"reg\" : 1e-5,\n",
    "         \"lr\" : 0.0001,\n",
    "         \"emb_dim\" : 64,\n",
    "         \"layers\" : '[64, 64]',\n",
    "         \"batch_size\" : 1024,\n",
    "         \"node_dropout\" : 0.1,\n",
    "         \"mess_dropout\" : 0.1,\n",
    "         \"k\" : 20, # 'k order of metric evaluation (e.g. NDCG@k)\n",
    "         \"eval_N\" : 1,\n",
    "         \"save_results\" : 1,\n",
    "         \"gpu_id\" : 0 ,\n",
    "         })\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(args.gpu_id) if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "class Data(object):\n",
    "    def __init__(self, path, batch_size):\n",
    "        self.path = '.'\n",
    "\n",
    "        #get number of users and items\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.neg_pools = {}\n",
    "\n",
    "        self.exist_users = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        # data_path는 사용자의 디렉토리에 맞게 설정해야 합니다.\n",
    "        data_path = '../data/train/train_ratings.csv'\n",
    "        df = pd.read_csv(data_path)\n",
    "        self.n_train = len(df)\n",
    "\n",
    "        item_ids = df['item'].unique() # 아이템 고유 번호 리스트\n",
    "        user_ids = df['user'].unique() # 유저 고유 번호 리스트\n",
    "        num_item, num_user = len(item_ids), len(user_ids)\n",
    "        num_batch = num_user // batch_size\n",
    "        \n",
    "        # user, item indexing\n",
    "        # item2idx = pd.Series(data=np.arange(len(item_ids))+1, index=item_ids) # item re-indexing (1~num_item) ; 아이템을 1부터 설정하는이유? 0을 아무것도 아닌 것으로 blank 하기 위해서\n",
    "        item2idx = pd.Series(data=np.arange(len(item_ids)), index=item_ids) # item re-indexing (0~num_item-1) ; 아이템을 1부터 설정하는이유? 0을 아무것도 아닌 것으로 blank 하기 위해서\n",
    "        user2idx = pd.Series(data=np.arange(len(user_ids)), index=user_ids) # user re-indexing (0~num_user-1)\n",
    "\n",
    "        # dataframe indexing\n",
    "        df = pd.merge(df, pd.DataFrame({'item': item_ids, 'item_idx': item2idx[item_ids].values}), on='item', how='inner')\n",
    "        df = pd.merge(df, pd.DataFrame({'user': user_ids, 'user_idx': user2idx[user_ids].values}), on='user', how='inner')\n",
    "        df.sort_values(['user_idx', 'time'], inplace=True)\n",
    "        del df['item'], df['user']\n",
    "\n",
    "        self.n_items = num_item\n",
    "        self.n_users = num_user\n",
    "        self.exist_users = list(df['user_idx'].unique())\n",
    "\n",
    "\n",
    "        # create interactions/ratings matrix 'R' # dok = dictionary of keys\n",
    "        print('Creating interaction matrices R_train and R_test...')\n",
    "        t1 = time()\n",
    "        self.R_train = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32) \n",
    "        self.R_test = sp.dok_matrix((self.n_users, self.n_items), dtype=np.float32)\n",
    "\n",
    "        self.train_items, self.test_set = {}, {}\n",
    "        \n",
    "        items = df.groupby(\"user_idx\")[\"item_idx\"].apply(list) # 유저 아이디 상관 없이, 순서대로 \n",
    "        for uid, item in tqdm.tqdm(enumerate(items)):\n",
    "            for i in item:\n",
    "                self.R_train[uid, i] = 1.\n",
    "            self.train_items[uid] = item\n",
    "\n",
    "        print('Complete. Interaction matrices R_train and R_test created in', time() - t1, 'sec')        \n",
    "\n",
    "\n",
    "    # if exist, get adjacency matrix\n",
    "    def get_adj_mat(self):\n",
    "        try:\n",
    "            t1 = time()\n",
    "            adj_mat = sp.load_npz(self.path + '/s_adj_mat.npz')\n",
    "            print('Loaded adjacency-matrix (shape:', adj_mat.shape,') in', time() - t1, 'sec.')\n",
    "\n",
    "        except Exception:\n",
    "            print('Creating adjacency-matrix...')\n",
    "            adj_mat = self.create_adj_mat()\n",
    "            sp.save_npz(self.path + '/s_adj_mat.npz', adj_mat)\n",
    "        return adj_mat\n",
    "    \n",
    "    # create adjancency matrix\n",
    "    def create_adj_mat(self):\n",
    "        t1 = time()\n",
    "        \n",
    "        adj_mat = sp.dok_matrix((self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        R = self.R_train.tolil() # to list of lists\n",
    "\n",
    "        adj_mat[:self.n_users, self.n_users:] = R\n",
    "        adj_mat[self.n_users:, :self.n_users] = R.T\n",
    "        adj_mat = adj_mat.todok()\n",
    "        print('Complete. Adjacency-matrix created in', adj_mat.shape, time() - t1, 'sec.')\n",
    "\n",
    "        t2 = time()\n",
    "\n",
    "        # normalized adjacency matrix\n",
    "        def normalized_adj_single(adj):\n",
    "            \n",
    "            ### 논문 수식 (8) ###\n",
    "            rowsum = np.array(adj.sum(1))                \n",
    "            d_inv = np.power(rowsum, -.5).flatten()     \n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)                 \n",
    "            norm_adj = d_mat_inv.dot(adj).dot(d_mat_inv) \n",
    "            \n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        print('Transforming adjacency-matrix to Normalized-adjacency matrix...')\n",
    "        ngcf_adj_mat = normalized_adj_single(adj_mat)\n",
    "\n",
    "        print('Complete. Transformed adjacency-matrix to Normalized-adjacency matrix in', time() - t2, 'sec.')\n",
    "        return ngcf_adj_mat.tocsr()\n",
    "\n",
    "    # create collections of N items that users never interacted with\n",
    "    def negative_pool(self):\n",
    "        t1 = time()\n",
    "        for u in self.train_items.keys():\n",
    "            neg_items = list(set(range(self.n_items)) - set(self.train_items[u]))\n",
    "            pools = [rd.choice(neg_items) for _ in range(100)]\n",
    "            self.neg_pools[u] = pools\n",
    "        print('refresh negative pools', time() - t1)\n",
    "\n",
    "    # sample data for mini-batches\n",
    "    def sample(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_items[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
    "                if neg_id not in self.train_items[u] and neg_id not in neg_items:\n",
    "                    neg_items.append(neg_id)\n",
    "            return neg_items\n",
    "\n",
    "        def sample_neg_items_for_u_from_pools(u, num):\n",
    "            neg_items = list(set(self.neg_pools[u]) - set(self.train_items[u]))\n",
    "            return rd.sample(neg_items, num)\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_num_users_items(self):\n",
    "        return self.n_users, self.n_items\n",
    "\n",
    "    def print_statistics(self):\n",
    "        print('n_users=%d, n_items=%d' % (self.n_users, self.n_items))\n",
    "        print('n_interactions=%d' % (self.n_train + self.n_test))\n",
    "        print('n_train=%d, n_test=%d, sparsity=%.5f' % (self.n_train, self.n_test, (self.n_train + self.n_test)/(self.n_users * self.n_items)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(log_value, best_value, stopping_step, flag_step, expected_order='asc'):\n",
    "    \"\"\"\n",
    "    Check if early_stopping is needed\n",
    "    Function copied from original code\n",
    "    \"\"\"\n",
    "    assert expected_order in ['asc', 'des']\n",
    "    if (expected_order == 'asc' and log_value >= best_value) or (expected_order == 'des' and log_value <= best_value):\n",
    "        stopping_step = 0\n",
    "        best_value = log_value\n",
    "    else:\n",
    "        stopping_step += 1\n",
    "\n",
    "    if stopping_step >= flag_step:\n",
    "        print(\"Early stopping at step: {} log:{}\".format(flag_step, log_value))\n",
    "        should_stop = True\n",
    "    else:\n",
    "        should_stop = False\n",
    "\n",
    "    return best_value, stopping_step, should_stop\n",
    "\n",
    "\n",
    "def train(model, data_generator, optimizer):\n",
    "    \"\"\"\n",
    "    Train the model PyTorch style\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    model: PyTorch model\n",
    "    data_generator: Data object\n",
    "    optimizer: PyTorch optimizer\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    n_batch = data_generator.n_train // data_generator.batch_size + 1\n",
    "    running_loss=0\n",
    "    for _ in range(n_batch):\n",
    "        u, i, j = data_generator.sample()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(u,i,j)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def split_matrix(X, n_splits=100):\n",
    "    \"\"\"\n",
    "    Split a matrix/Tensor into n_folds (for the user embeddings and the R matrices)\n",
    "\n",
    "    Arguments:\n",
    "    ---------\n",
    "    X: matrix to be split\n",
    "    n_folds: number of folds\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    splits: split matrices\n",
    "    \"\"\"\n",
    "    splits = []\n",
    "    chunk_size = X.shape[0] // n_splits\n",
    "    for i in range(n_splits):\n",
    "        start = i * chunk_size\n",
    "        end = X.shape[0] if i == n_splits - 1 else (i + 1) * chunk_size\n",
    "        splits.append(X[start:end])\n",
    "    return splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndcg_k(pred_items, test_items, test_indices, k):\n",
    "    \"\"\"\n",
    "    Compute NDCG@k\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    pred_items: binary tensor with 1s in those locations corresponding to the predicted item interactions\n",
    "    test_items: binary tensor with 1s in locations corresponding to the real test interactions\n",
    "    test_indices: tensor with the location of the top-k predicted items\n",
    "    k: k'th-order \n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    NDCG@k\n",
    "    \"\"\"\n",
    "    r = (test_items * pred_items).gather(1, test_indices)\n",
    "    f = torch.from_numpy(np.log2(np.arange(2, k+2))).float().to(device)\n",
    "    \n",
    "    dcg = (r[:, :k]/f).sum(1)                                               \n",
    "    dcg_max = (torch.sort(r, dim=1, descending=True)[0][:, :k]/f).sum(1)   \n",
    "    ndcg = dcg/dcg_max                                                     \n",
    "    \n",
    "    ndcg[torch.isnan(ndcg)] = 0\n",
    "    return ndcg\n",
    "\n",
    "def eval_model(u_emb, i_emb, Rtr, Rte, k):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    u_emb: User embeddings\n",
    "    i_emb: Item embeddings\n",
    "    Rtr: Sparse matrix with the training interactions\n",
    "    Rte: Sparse matrix with the testing interactions\n",
    "    k : kth-order for metrics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result: Dictionary with lists correponding to the metrics at order k for k in Ks\n",
    "    \"\"\"\n",
    "    # split matrices\n",
    "    ue_splits = split_matrix(u_emb)\n",
    "    tr_splits = split_matrix(Rtr)\n",
    "    te_splits = split_matrix(Rte)\n",
    "\n",
    "    recall_k, ndcg_k= [], []\n",
    "    # compute results for split matrices\n",
    "    for ue_f, tr_f, te_f in zip(ue_splits, tr_splits, te_splits):\n",
    "\n",
    "        scores = torch.mm(ue_f, i_emb.t())\n",
    "\n",
    "        test_items = torch.from_numpy(te_f.todense()).float().to(device)\n",
    "        non_train_items = torch.from_numpy(1-(tr_f.todense())).float().to(device)\n",
    "        scores = scores * non_train_items\n",
    "\n",
    "        _, test_indices = torch.topk(scores, dim=1, k=k)\n",
    "        \n",
    "\n",
    "        pred_items = torch.zeros_like(scores).float()\n",
    "        pred_items.scatter_(dim=1, index=test_indices, src=torch.ones_like(test_indices).float().to(device))\n",
    "\n",
    "        topk_preds = torch.zeros_like(scores).float()\n",
    "        topk_preds.scatter_(dim=1, index=test_indices[:, :k], src=torch.ones_like(test_indices).float())\n",
    "        \n",
    "        TP = (test_items * topk_preds).sum(1)                      \n",
    "        rec = TP/test_items.sum(1)\n",
    "   \n",
    "        ndcg = compute_ndcg_k(pred_items, test_items, test_indices, k)\n",
    "\n",
    "        recall_k.append(rec)\n",
    "        ndcg_k.append(ndcg)\n",
    "\n",
    "    return torch.cat(recall_k).mean(), torch.cat(ndcg_k).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict(\n",
    "        {\"data_dir\": './data/', \n",
    "         \"dataset\": \"ml-100k_ngcf\", \n",
    "         \"results_dir\": '', \n",
    "         \"n_epochs\" : 40,# 400,\n",
    "         \"reg\" : 1e-5,\n",
    "         \"lr\" : 0.0001,\n",
    "         \"emb_dim\" : 64,\n",
    "         \"layers\" : '[64, 64]',\n",
    "         \"batch_size\" : 1024,\n",
    "         \"node_dropout\" : 0.1,\n",
    "         \"mess_dropout\" : 0.1,\n",
    "         \"k\" : 20, # 'k order of metric evaluation (e.g. NDCG@k)\n",
    "         \"eval_N\" : 1,\n",
    "         \"save_results\" : 1,\n",
    "         \"gpu_id\" : 0 ,\n",
    "         })\n",
    "\n",
    "device = torch.device(\"cuda:\" + str(args.gpu_id) if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read parsed arguments\n",
    "data_dir = args.data_dir\n",
    "dataset = args.dataset\n",
    "batch_size = args.batch_size\n",
    "layers = eval(args.layers)\n",
    "emb_dim = args.emb_dim\n",
    "lr = args.lr\n",
    "reg = args.reg\n",
    "mess_dropout = args.mess_dropout\n",
    "node_dropout = args.node_dropout\n",
    "k = args.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim, layers, reg, node_dropout, mess_dropout, adj_mtx):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize Class attributes\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.emb_dim = emb_dim\n",
    "        self.l_matrix = adj_mtx\n",
    "        self.l_plus_i_matrix = adj_mtx + sp.eye(adj_mtx.shape[0])\n",
    "        self.reg = reg\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(self.layers)\n",
    "        self.node_dropout = node_dropout\n",
    "        self.mess_dropout = mess_dropout\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weight_dict = self._init_weights()\n",
    "        print(\"Weights initialized.\")\n",
    "\n",
    "        # Create Matrix 'L+I', PyTorch sparse tensor of SP adjacency_mtx\n",
    "        self.L_plus_I = self._convert_sp_mat_to_sp_tensor(self.l_plus_i_matrix)\n",
    "        self.L = self._convert_sp_mat_to_sp_tensor(self.l_matrix)\n",
    "\n",
    "    # initialize weights\n",
    "    def _init_weights(self):\n",
    "        print(\"Initializing weights...\")\n",
    "        weight_dict = nn.ParameterDict()\n",
    "\n",
    "        initializer = torch.nn.init.xavier_uniform_\n",
    "        \n",
    "        weight_dict['user_embedding'] = nn.Parameter(initializer(torch.empty(self.n_users, self.emb_dim).to(device)))\n",
    "        weight_dict['item_embedding'] = nn.Parameter(initializer(torch.empty(self.n_items, self.emb_dim).to(device)))\n",
    "\n",
    "        weight_size_list = [self.emb_dim] + self.layers\n",
    "\n",
    "        for k in range(self.n_layers):\n",
    "            weight_dict['W_one_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
    "            weight_dict['b_one_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
    "            \n",
    "            weight_dict['W_two_%d' %k] = nn.Parameter(initializer(torch.empty(weight_size_list[k], weight_size_list[k+1]).to(device)))\n",
    "            weight_dict['b_two_%d' %k] = nn.Parameter(initializer(torch.empty(1, weight_size_list[k+1]).to(device)))\n",
    "           \n",
    "        return weight_dict\n",
    "\n",
    "    # convert sparse matrix into sparse PyTorch tensor\n",
    "    def _convert_sp_mat_to_sp_tensor(self, X):\n",
    "        \"\"\"\n",
    "        Convert scipy sparse matrix to PyTorch sparse matrix\n",
    "\n",
    "        Arguments:\n",
    "        ----------\n",
    "        X = Adjacency matrix, scipy sparse matrix\n",
    "        \"\"\"\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        i = torch.LongTensor(np.mat([coo.row, coo.col]))\n",
    "        v = torch.FloatTensor(coo.data)\n",
    "        res = torch.sparse.FloatTensor(i, v, coo.shape).to(device)\n",
    "        return res\n",
    "\n",
    "    # apply node_dropout\n",
    "    def _droupout_sparse(self, X):\n",
    "        \"\"\"\n",
    "        Drop individual locations in X\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        X = adjacency matrix (PyTorch sparse tensor)\n",
    "        dropout = fraction of nodes to drop\n",
    "        noise_shape = number of non non-zero entries of X\n",
    "        \"\"\"\n",
    "        node_dropout_mask = ((self.node_dropout) + torch.rand(X._nnz())).floor().bool().to(device)\n",
    "        i = X.coalesce().indices()\n",
    "        v = X.coalesce()._values()\n",
    "        i[:,node_dropout_mask] = 0\n",
    "        v[node_dropout_mask] = 0\n",
    "        X_dropout = torch.sparse.FloatTensor(i, v, X.shape).to(X.device)\n",
    "\n",
    "        return  X_dropout.mul(1/(1-self.node_dropout))\n",
    "\n",
    "    def forward(self, u, i, j):\n",
    "        \"\"\"\n",
    "        Computes the forward pass\n",
    "        \n",
    "        Arguments:\n",
    "        ---------\n",
    "        u = user\n",
    "        i = positive item (user interacted with item)\n",
    "        j = negative item (user did not interact with item)\n",
    "        \"\"\"\n",
    "        # apply drop-out mask\n",
    "        L_plus_I_hat = self._droupout_sparse(self.L_plus_I) if self.node_dropout > 0 else self.L_plus_I\n",
    "        L_hat = self._droupout_sparse(self.L) if self.node_dropout > 0 else self.L\n",
    "        \n",
    "        # 논문 수식 (1)\n",
    "        # user, item 임베딩 벡터를 연결한다. \n",
    "        ego_embeddings = torch.cat([self.weight_dict['user_embedding'], self.weight_dict['item_embedding']], 0)\n",
    "\n",
    "        final_embeddings = [ego_embeddings]\n",
    "\n",
    "        # forward pass for 'n' propagation layers\n",
    "        for k in range(self.n_layers):\n",
    "            \n",
    "            \n",
    "            ########## Fill below ###########\n",
    "            ### 논문 수식 (7) ###\n",
    "            \n",
    "            # (L+I)E\n",
    "            side_L_plus_I_embeddings = torch.sparse.mm(L_plus_I_hat, ego_embeddings) #힌트 : use torch.sparse.mm \n",
    "            \n",
    "            # (L+I)EW_1 + b_1\n",
    "            simple_embeddings = torch.matmul(side_L_plus_I_embeddings, self.weight_dict['W_one_%d' %k]) + self.weight_dict['b_one_%d' % k] #힌트 : use torch.matmul, self.weight_dict['W_one_%d' % k], self.weight_dict['b_one_%d' % k]\n",
    "            \n",
    "            # LE\n",
    "            side_L_embeddings = torch.sparse.mm(L_hat, ego_embeddings) #힌트 : use torch.sparse.mm                                \n",
    "            \n",
    "            # LEE\n",
    "            interaction_embeddings = torch.mul(side_L_embeddings, ego_embeddings)            #힌트 : use torch.mul\n",
    "                                             \n",
    "            # LEEW_2 + b_2\n",
    "            interaction_embeddings = torch.matmul(interaction_embeddings, self.weight_dict['W_two_%d' % k]) + self.weight_dict['b_two_%d' % k] #힌트 : use torch.matmul, self.weight_dict['W_two_%d' % k], self.weight_dict['b_two_%d' % k]\n",
    "\n",
    "            # non-linear activation \n",
    "            ego_embeddings = simple_embeddings + interaction_embeddings #힌트: use simple_embeddings, interaction_embeddings\n",
    "            \n",
    "            ########## Fill above ###########\n",
    "            \n",
    "            \n",
    "            # add message dropout\n",
    "            mess_dropout_mask = nn.Dropout(self.mess_dropout)\n",
    "            ego_embeddings = mess_dropout_mask(ego_embeddings)\n",
    "\n",
    "            # Perform L2 normalization\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "            \n",
    "            ### 논문 수식 (9) ###\n",
    "            final_embeddings.append(norm_embeddings)                                            \n",
    "\n",
    "        \n",
    "        final_embeddings = torch.cat(final_embeddings, 1)                           \n",
    "    \n",
    "        # back to user/item dimension\n",
    "        u_final_embeddings, i_final_embeddings = final_embeddings.split([self.n_users, self.n_items], 0)\n",
    "\n",
    "        self.u_final_embeddings = nn.Parameter(u_final_embeddings)\n",
    "        self.i_final_embeddings = nn.Parameter(i_final_embeddings)\n",
    "        \n",
    "        u_emb = u_final_embeddings[u] # user embeddings\n",
    "        p_emb = i_final_embeddings[i] # positive item embeddings\n",
    "        n_emb = i_final_embeddings[j] # negative item embeddings\n",
    "    \n",
    "        \n",
    "        ########## Fill below ###########\n",
    "        ### 논문 수식 (10) ###\n",
    "        \n",
    "        y_ui =  sum(torch.mul(u_emb, p_emb))               # 힌트 : use torch.mul, sum() method                             \n",
    "        y_uj =  sum(torch.mul(u_emb, n_emb))               # 힌트 : use torch.mul, sum() method \n",
    "        \n",
    "        ########## Fill above ########### \n",
    "        \n",
    "        \n",
    "        \n",
    "        ########## Fill below ########### \n",
    "        ### 논문 수식 (11) ###\n",
    "        \n",
    "        log_prob =  torch.log(torch.sigmoid(y_ui-y_uj)).mean()           # 힌트 : use torch.log, torch.sigmoid, mean() method\n",
    "        bpr_loss = -log_prob        \n",
    "        if self.reg > 0.:\n",
    "            l2norm = (torch.sum(u_emb**2)/2. + torch.sum(p_emb**2)/2. + torch.sum(n_emb**2)/2.) / u_emb.shape[0]\n",
    "            l2reg = self.reg*l2norm            # FILL HERE #\n",
    "            bpr_loss += l2reg\n",
    "        \n",
    "        ########## Fill above ###########\n",
    "        \n",
    "        \n",
    "        return bpr_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read parsed arguments\n",
    "data_dir = args.data_dir\n",
    "dataset = args.dataset\n",
    "batch_size = args.batch_size\n",
    "layers = eval(args.layers)\n",
    "emb_dim = args.emb_dim\n",
    "lr = args.lr\n",
    "reg = args.reg\n",
    "mess_dropout = args.mess_dropout\n",
    "node_dropout = args.node_dropout\n",
    "k = args.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interaction matrices R_train and R_test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31360it [00:57, 545.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete. Interaction matrices R_train and R_test created in 58.93110489845276 sec\n",
      "Loaded adjacency-matrix (shape: (38167, 38167) ) in 0.0034456253051757812 sec.\n",
      "Initializing weights...\n",
      "Weights initialized.\n",
      "Start at 2022-04-01 04:48:18.412060\n",
      "Using cuda:0 for computations\n",
      "Params on CUDA: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate the Normalized-adjacency matrix\n",
    "data_generator = Data(path=data_dir + dataset, batch_size=batch_size)\n",
    "adj_mtx = data_generator.get_adj_mat()\n",
    "\n",
    "# create model name and save\n",
    "modelname =  \"NGCF\" + \\\n",
    "    \"_bs_\" + str(batch_size) + \\\n",
    "    \"_nemb_\" + str(emb_dim) + \\\n",
    "    \"_layers_\" + str(layers) + \\\n",
    "    \"_nodedr_\" + str(node_dropout) + \\\n",
    "    \"_messdr_\" + str(mess_dropout) + \\\n",
    "    \"_reg_\" + str(reg) + \\\n",
    "    \"_lr_\"  + str(lr)\n",
    "\n",
    "# create NGCF model\n",
    "model = NGCF(data_generator.n_users, \n",
    "             data_generator.n_items,\n",
    "             emb_dim,\n",
    "             layers,\n",
    "             reg,\n",
    "             node_dropout,\n",
    "             mess_dropout,\n",
    "             adj_mtx)\n",
    "\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# current best metric\n",
    "cur_best_metric = 0\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "# Set values for early stopping\n",
    "cur_best_loss, stopping_step, should_stop = 1e3, 0, False\n",
    "today = datetime.now()\n",
    "\n",
    "print(\"Start at \" + str(today))\n",
    "print(\"Using \" + str(device) + \" for computations\")\n",
    "print(\"Params on CUDA: \" + str(next(model.parameters()).is_cuda))\n",
    "\n",
    "results = {\"Epoch\": [],\n",
    "           \"Loss\": [],\n",
    "           \"Recall\": [],\n",
    "           \"NDCG\": [],\n",
    "           \"Training Time\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training time: 477.43s, Loss: 710.7754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [09:26<6:08:29, 566.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate current model:\n",
      " Epoch: 0, Validation time: 89.48s \n",
      " Loss: 710.7754: \n",
      " Recall@20: nan \n",
      " NDCG@20: 0.0000\n",
      "Epoch: 1, Training time: 477.50s, Loss: 19.5138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [18:54<5:59:23, 567.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate current model:\n",
      " Epoch: 1, Validation time: 90.36s \n",
      " Loss: 19.5138: \n",
      " Recall@20: nan \n",
      " NDCG@20: 0.0000\n",
      "Epoch: 2, Training time: 478.49s, Loss: 0.7031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [28:21<5:49:48, 567.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate current model:\n",
      " Epoch: 2, Validation time: 88.53s \n",
      " Loss: 0.7031: \n",
      " Recall@20: nan \n",
      " NDCG@20: 0.0000\n",
      "Epoch: 3, Training time: 476.87s, Loss: 0.2622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [37:47<5:40:01, 566.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate current model:\n",
      " Epoch: 3, Validation time: 88.96s \n",
      " Loss: 0.2622: \n",
      " Recall@20: nan \n",
      " NDCG@20: 0.0000\n",
      "Epoch: 4, Training time: 473.29s, Loss: 0.2343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [47:10<7:04:31, 707.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate current model:\n",
      " Epoch: 4, Validation time: 89.24s \n",
      " Loss: 0.2343: \n",
      " Recall@20: nan \n",
      " NDCG@20: 0.0000\n",
      "Early stopping at step: 5 log:nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(args.n_epochs)):\n",
    "\n",
    "    t1 = time()\n",
    "    loss = train(model, data_generator, optimizer)\n",
    "    training_time = time()-t1\n",
    "    print(\"Epoch: {}, Training time: {:.2f}s, Loss: {:.4f}\".\n",
    "        format(epoch, training_time, loss))\n",
    "\n",
    "    # print test evaluation metrics every N epochs (provided by args.eval_N)\n",
    "    if epoch % args.eval_N  == (args.eval_N - 1):\n",
    "        with torch.no_grad():\n",
    "            t2 = time()\n",
    "            recall, ndcg = eval_model(model.u_final_embeddings.detach(),\n",
    "                                      model.i_final_embeddings.detach(),\n",
    "                                      data_generator.R_train,\n",
    "                                      data_generator.R_test,\n",
    "                                      k)\n",
    "        print(\n",
    "            \"Evaluate current model:\\n\",\n",
    "            \"Epoch: {}, Validation time: {:.2f}s\".format(epoch, time()-t2),\"\\n\",\n",
    "            \"Loss: {:.4f}:\".format(loss), \"\\n\",\n",
    "            \"Recall@{}: {:.4f}\".format(k, recall), \"\\n\",\n",
    "            \"NDCG@{}: {:.4f}\".format(k, ndcg)\n",
    "            )\n",
    "\n",
    "        cur_best_metric, stopping_step, should_stop = \\\n",
    "        early_stopping(recall, cur_best_metric, stopping_step, flag_step=5)\n",
    "\n",
    "        # save results in dict\n",
    "        results['Epoch'].append(epoch)\n",
    "        results['Loss'].append(loss)\n",
    "        results['Recall'].append(recall.item())\n",
    "        results['NDCG'].append(ndcg.item())\n",
    "        results['Training Time'].append(training_time)\n",
    "    else:\n",
    "        # save results in dict\n",
    "        results['Epoch'].append(epoch)\n",
    "        results['Loss'].append(loss)\n",
    "        results['Recall'].append(None)\n",
    "        results['NDCG'].append(None)\n",
    "        results['Training Time'].append(training_time)\n",
    "\n",
    "    if should_stop == True: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8892ab59d46dba3f4efad217c937d392d78b127da621344609ec3a012e116b8b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
